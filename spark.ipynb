from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, desc

# Generate a sample CSV file for testing
import pandas as pd
import random
from datetime import datetime, timedelta

num_rows = 10000
categories = ["Electronics", "Clothing", "Books", "Home", "Toys"]
data = {
    "ID": range(1, num_rows + 1),
    "Category": [random.choice(categories) for _ in range(num_rows)],
    "Price": [round(random.uniform(5, 500), 2) for _ in range(num_rows)],
    "Timestamp": [(datetime.now() - timedelta(days=random.randint(0, 365))).strftime("%Y-%m-%d %H:%M:%S") for _ in range(num_rows)]
}

# Create DataFrame
df_sample = pd.DataFrame(data)

# Save as CSV
dataset_path = "large_dataset.csv"
df_sample.to_csv(dataset_path, index=False)

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Big Data Analysis") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()

# Load dataset
df = spark.read.csv(dataset_path, header=True, inferSchema=True)

# Show dataset schema
df.printSchema()

# Display first few rows
df.show(5)

# Basic statistics
df.describe().show()

# Data aggregation example: Counting occurrences of a categorical column (e.g., category)
if "Category" in df.columns:
    df.groupBy("Category").agg(count("*").alias("count")).orderBy(desc("count")).show()

# Example of numerical analysis
df.select(avg(col("Price")).alias("average_price")).show()

# Stop Spark session
spark.stop()
